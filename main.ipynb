{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNeuralNetwork:\n",
    "    def __init__(self, name, input_shape):\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "        self.random_normal = RandomNormal(stddev=.02)\n",
    "        self.input_layer = Input(shape=input_shape)\n",
    "        self.output_layer = None\n",
    "\n",
    "    def model_summary(self):\n",
    "        if self.model:\n",
    "            print(self.model.summary())\n",
    "        else:\n",
    "            print(\"{} is not built yet\".format(self.name))\n",
    "            \n",
    "    #candidate for more template treatment\n",
    "        #conv transpose or not\n",
    "        #leaky relu, normal relu, or none\n",
    "        #etc\n",
    "    def create_conv_block(self, input_layer, output_space, kernal_size, instance_norm=True, relu='leaky', strides=(2, 2), transpose=False):\n",
    "        if transpose:\n",
    "            conv = Conv2DTranspose(output_space, \n",
    "                                   kernal_size, \n",
    "                                   strides=strides, \n",
    "                                   padding='same', \n",
    "                                   kernel_initializer=self.random_normal)(input_layer)\n",
    "        else:\n",
    "            conv = Conv2D(output_space, \n",
    "                          kernal_size, \n",
    "                          strides=strides, \n",
    "                          padding='same', \n",
    "                          kernel_initializer=self.random_normal)(input_layer)\n",
    "            \n",
    "        if instance_norm: conv = InstanceNormalization(axis=-1)(conv) \n",
    "        if relu == 'leaky': conv = LeakyReLU(alpha=.2)(conv)\n",
    "        if relu == 'normal': conv = Activation('relu')(conv)\n",
    "        return conv\n",
    "        \n",
    "        \n",
    "class Discriminator(BaseNeuralNetwork):\n",
    "    def __init__(self, name, input_shape):\n",
    "        super().__init__(name, input_shape)\n",
    "        \n",
    "    def paper_build(self):    \n",
    "        c = self.create_conv_block(self.input_layer, 64, (4, 4), instance_norm=False)\n",
    "        c = self.create_conv_block(c, 128, (4, 4))\n",
    "        c = self.create_conv_block(c, 256, (4, 4))\n",
    "        c = self.create_conv_block(c, 512, (4, 4))\n",
    "        c = self.create_conv_block(c, 512, (4, 4), strides=(1, 1))\n",
    "        out = self.create_conv_block(c, 1, (4, 4), instance_norm=False, relu=None, strides=(1, 1))\n",
    "        self.output_layer = out\n",
    "    \n",
    "    def compile_model(self):\n",
    "        if self.output_layer is not None:\n",
    "            self.model = Model(self.input_layer, self.output_layer)\n",
    "            self.model.name = self.name\n",
    "            self.model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "        else:\n",
    "            print('No output layers provided for {}'.format(self.name))\n",
    "            \n",
    "class Generator(BaseNeuralNetwork):\n",
    "    def __init__(self, name, input_shape):\n",
    "        super().__init__(name, input_shape)\n",
    "        \n",
    "    def create_resnet_block(self, input_layer, output_space):\n",
    "        #first layer\n",
    "        res = self.create_conv_block(input_layer, output_space, (3, 3), relu='normal', strides=(1, 1))\n",
    "        # second convolutional layer\n",
    "        res = self.create_conv_block(res, output_space, (3, 3), relu=None, strides=(1, 1))\n",
    "        # concatenate merge channel-wise with input layer\n",
    "        res = Concatenate()([res, input_layer])\n",
    "        return res\n",
    "    \n",
    "    def paper_build(self):\n",
    "        c = self.create_conv_block(self.input_layer, 64, (7, 7), relu='normal', strides=(1, 1))\n",
    "        c = self.create_conv_block(c, 128, (3, 3), relu='normal')\n",
    "        c = self.create_conv_block(c, 256, (3, 3), relu='normal')\n",
    "        for _ in range(9):\n",
    "            c = self.create_resnet_block(c, 256)\n",
    "        c = self.create_conv_block(c, 128, (3, 3), relu='normal', transpose=True)\n",
    "        c = self.create_conv_block(c, 64, (3, 3), relu='normal', transpose=True)\n",
    "        c = self.create_conv_block(c, 3, (7, 7), relu=None, strides=(1, 1))\n",
    "        out = Activation('tanh')(c)\n",
    "        self.output_layer = out\n",
    "    \n",
    "    def compile_model(self):\n",
    "        if self.output_layer is not None:\n",
    "            self.model = Model(self.input_layer, self.output_layer)\n",
    "            self.model.name = self.name\n",
    "        else:\n",
    "            print('No output layers provided for {}'.format(self.name))\n",
    "            \n",
    "    def generate_fake_samples(self, dataset, patch_shape):\n",
    "        X = self.model.predict(dataset)\n",
    "        #fake images have label of zero\n",
    "        y = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "class Composite():\n",
    "    def __init__(self, name, g1, d1, g2, input_shape):\n",
    "        g_model_1 = g1.model\n",
    "        d_model = d1.model\n",
    "        g_model_2 = g2.model\n",
    "        \n",
    "        #We are only training the first generator\n",
    "        g_model_1.trainable = True\n",
    "        d_model.trainable = False\n",
    "        g_model_2.trainable = False\n",
    "        \n",
    "        #discriminator element\n",
    "        #generator 1 creates an image for domain 1 and discriminator 1 try to see if it was real or fake\n",
    "        input_gen = Input(shape=input_shape)\n",
    "        gen1_out = g_model_1(input_gen)\n",
    "        output_d = d_model(gen1_out)\n",
    "        \n",
    "        #identity element\n",
    "        #generator 1 receives a real image from domain 1 and trys not to change it\n",
    "        input_id = Input(shape=input_shape)\n",
    "        output_id = g_model_1(input_id)\n",
    "        \n",
    "        #forward cycle\n",
    "        #generator 2 receives an image from generator 1 and tries to change it back to domain 2\n",
    "        output_f = g_model_2(gen1_out)\n",
    "        \n",
    "        #backward cycle\n",
    "        #generator 2 receives real image from domain 1 and gives it to generator 1 who tries to change it back to domain 1\n",
    "        gen2_out = g_model_2(input_id)\n",
    "        output_b = g_model_1(gen2_out)\n",
    "        \n",
    "        self.model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=Adam(lr=.0002, beta_1=.5))\n",
    "        \n",
    "    def model_summary(self):\n",
    "        if self.model:\n",
    "            print(self.model.summary())\n",
    "        else:\n",
    "            print(\"{} is not built yet\".format(self.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (256, 256, 3)\n",
    "d1 = Discriminator('Impr-Discriminator', image_shape)\n",
    "g1 = Generator('Impr-Generator', image_shape)\n",
    "d2 = Discriminator('Photo-Discriminator', image_shape)\n",
    "g2 = Generator('Photo-Generator', image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.paper_build()\n",
    "g1.paper_build()\n",
    "d2.paper_build()\n",
    "g2.paper_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.compile_model()\n",
    "g1.compile_model()\n",
    "d2.compile_model()\n",
    "g2.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2p = Composite('Impr to Photo', g1, d1, g2, image_shape)\n",
    "p2i = Composite('Photo to Impr', g2, d2, g1, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2p.compile_model()\n",
    "p2i.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = ImageDataGenerator(rescale=1.0/255,\n",
    "                                  rotation_range=15,\n",
    "                                  width_shift_range=.2,\n",
    "                                  height_shift_range=.1,\n",
    "                                  zoom_range=.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(path):\n",
    "    all_files = os.listdir(path)\n",
    "    imgs = np.zeros((1300, 256, 256, 3))\n",
    "    for i in range(1300):\n",
    "    #for i, file in enumerate(all_files):\n",
    "        file = all_files[i]\n",
    "        print(i, end='\\r')\n",
    "        image = np.array(load_img(path+'/'+ file, target_size=(256, 256)))\n",
    "        image = (image - 127.5) / 127.5\n",
    "        imgs[i] = image\n",
    "        #imgs.append(image)\n",
    "        del image\n",
    "        \n",
    "    print('\\nend')\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1299\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "impr_imgs = load_samples('monet-paintings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1299\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "photo_imgs = load_samples('landscape-pictures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 256, 256, 3)\n",
      "[-0.98431373 -0.79607843 -0.76470588]\n",
      "(1300, 256, 256, 3)\n",
      "[0.67058824 0.75686275 0.91372549]\n"
     ]
    }
   ],
   "source": [
    "print(photo_imgs.shape)\n",
    "print(photo_imgs[0][0][0])\n",
    "print(impr_imgs.shape)\n",
    "print(impr_imgs[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('monet-paintings.npy', impr_imgs)\n",
    "np.save('landscape-photos.npy', photo_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABwCAYAAAC9zaPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPLUlEQVR4nO2d3Za0KAxFYda8/yMPc1FtdyrmD0QEOXvWN12liBTiIYSguZSSAAAAjOGfpwsAAAA7AdEFAICBQHQBAGAgEF0AABgIRBcAAAbyr7k3Zze0IaeUSuB7Jtus9DV5X+Eoj5Z/9NzeMfwctWX8PbaUbCStyzdwXcEIcirlv27XNaXztc25a/avYFTEVlHuWVN0qWDyYtZeSu1neoKrnccTNC7yVj7Sfv67rfJrwisJsJWXlDd4M7jCI5klPDbkXpAEtwjbedorfSwVM/qPnp+eJwufo+XQ9lMx5n/psTQdLaMk5HQ/AGAM9wtuZp/1O7zJp1uUz1I6bkVygYzA01NLkud9lcz+HXDr9dhPt/N60Szsnm4SADTgWvhwr+BKY277fKZ7QfPFekNlabgdLVpEkEaLluSq8Nwamosh4g7hxwFQCwT3j5zzjcJbr2L2RFqSRcYrvuXnPL4nY79FrZXcC6msWkdCrWDJFSMJcS+3DNgbiK0MrZdSygNC/IcrujyLqJUXteikc80oRK2Weu1kmjUyAKCGQ1zAN1Kd1NTTVbF2RTcaRhUNl7KiIaR9szUZzd1ild2LgKB47gsAokBwbVrr52q9hixdK1rhgIqFNATX0qbkC/esSJN4XmiatI2KM69DWLwAvItQnC7fdsBFQRLRxLbxYyV3wuxi24uIBQyxBWA9LBdEOGSMi6vme+Vp6V9tht6KbX0LlgVvhaYBAN6FKbrWhJDmCtAmkiSXgxYT+zYik4dSujfXCQC74lq6mk9WC5uy3BEtCyPeQNRtEo0SAQDMjTXZ5oquJaJ0v2ax7jxcluJzqR874mbg2wEAa2OKriQKnv+1NT73TXgxuJHjIjG9AOxGbYxsKWWaB90chELGDjxfreRy2ElsD6T4ZWuF2pEupb1HBquza3u/Gy6a2qIPni7nPGWscngizUojiQwVj90ERIqv9erAc+OA+cE1u4eocNJ09y7zvUZ1nK6030uzG3zyUFutpnVUKe3XUQFg0Sq8M1Jt6cLXGIdHanir8KQmMmezAVfAfbM3VSFj1hJX/v3tsbe1aCMCKboBvBuEA+6N616gf1Pyl+/utpQ3CvfxeiOGiOsGrAuEV+ejLSUV7o9L+RW64roXLOsMK6nq4SMAyc+L+pufHoKJ6yyTS0qnObAX9VAhS9eLvaUiDGvXR5pYS0l3Nbyovb2GK2171471iCbIKaeUyXc24VW0Fl/UPb/5eM8QphENVoTDnZNwIUtXWsoricaODakF/uwJui2ls+sG9foudruefIFCSefvJPWl8/ydT9p2jve1y6zlXc6WeAXhxRGaj1FbMAFiaKKKugRPUBPb6lmD4byKYd02cxbeK3mc82kX3qoVaRpwKVxDmoiU9oM1kFxDK9wXLUtsQT2h5+lqbgS+3HeFhjUbkruG1iWa9RqcJtqFz0e6O5d6/w1/z7l7w2kwhipLV2pMENp+SNbRmrfCuiVvRYo84Rav1LneWUvfQvo5+2fTXtdmNsJvjsBlGs9KQ9Nv7mkts7RBaV5DEtwDPgpc73qCnoRWpNHeGgH792PF8u5M7YKRSLqautUWtmiuNX7f8HzGXtdnzgrOhB5ibvXaoD9Pdmwr3JJW++Mx4166KJolG10az/N5pp5XuLrvp+oh5imhvxxBZv9G44UG9j7XlbTaghJp3/FXm/SyzntFwKXJNBgu+xJ+yhiGu89xxw2qzaJri1/uEgkvX6lskXJpftSWCeAeoXx8QQzuoX0JTaRJz1ZAT30/d/rQPb8xXxX3FJJfVAuno2ImWbnH55a2a/lna/Jbd3IU9KI5ZAzchyUmvc9jWV6SoDw1NPYmqixLtmfkQI/j4V7YG9eniyD996IF7lvXekaxeMrvbX2PHAP2JGTpomceiySGdyDNpHNLDMNhGcnlJkHrGL5ckFJAdCXf2KhJFnD/TSp1qFQkcG3P1BghmtsD4rsv4df1cMGVGhEaUjta3d0pfFZIoPV5d65eD3Roe9MUvUDRYjpxk9rwWXar87rbvSB9154TgOt6HYjt3oTidLUgcz4rfGdjoo/uzPn7+ypIYU+ckTGyB1aZNJ8vAKCN0DJgLeSGT7iU9D106rl67fthwunSk9tHYy0y0SbNetefBJ/giboTFuzvAJiGsE9XWyCh3Zya33dnLMHVhu53R45o19CKz4WvF4B23DhdbaWSlFYTYi3q4c1Y/k8ePuQN7++Gdo5euWk6vh0A4GP7dIser1njc+QrhyyrLhn7Z4e7WDy0kcDIiTRa15pryOpQvREPAOAb272QxY8n14E1E3/8ldwNPA2PD13hRuadD51QjPpreQjRCH/ucR7+T7pWUlmiiwMAAN9UPdrRugGtaAYe2aAJMD+HdiM/MbTVzqkJqwQXNauTotvvFDQ+qtAmRlfrDAGYlaoVadI+PgNuEU3L01lW1SgLq+acfNJJEy2eJjKx1hPpWlg+e+kzAKAOU3Q9gYzMfFuTaJqo0jxWucG5qFrWKq8LT+TuelOrVjYAwH2EX0zJLTG6XUqjiQq/0d9wk0cnCCnc/0vTniYUV1wJAgAQCS2OiKKJS82M/opoFqMlqF5eWh4AgLWpeuCN5IuUxFQLK9oBbzRw4Pm0I+nAPOA6gSjVy4APpNhNaVJmdGOcofFrIXN0nxUWxuNnwfzsYlSA64RCxqxgfS/edHRjnKnxWxNmUnSGFPMLAHgXYffC8VmLTFgtWH6UFSlNmGkuGW4Fv9kP/kYwMgERwq9gl9DiTp/GK/ddN4cX03wghY9JoWZWSB2YD1wnECH8wBtpCDyV2JKwKq88dw3drTy1fdIqNLoNK8AAeBeupStFKmgxto+Kw6LKZLkeYOnOxaJNDExGaCItahk+Kw4lfFM88eyGCJg8mxtcG9CD0Iq0qdwIHZhxyK7F8i5X58sV+Blma39gHJdfTDkDLQ149qH7zGUz2UxNWn/uktcWdCH87IWU5r2fos9xmLX8Ergp1wDXCdRSJbqrN7CVyj+8g8BDdVRW6qzB/FSJ7oxcuSFmlpnhZVvp9cqDqX12BgAWS4julQbeeixuKnDgPTO6Z77g/SwhuimNn7CY2QoGY7mrLaCN7ckyoqs10NaGaz3EB4AoWrtRt6Ohbc8SomvF1Xpva7BeNUTTwurYl7vmBUS3BBra9rgvppyF2ucaRMPIommeAobR/Yx+DgfYmy6W7qrPXKBDvVndDbh5x1AShv5gDNWiKw3b+UNx+JsP7mzLuSH332XNRNFmfB4w3h4xjpww9AdjqBZdadguCbD0yhqKtz1CSanpTvEes1hbjl5Ir/dJaY4OAPQDHeneNLkXNCuWP89AevSjJigRwePn7S1GT4vbjNY26A+u696EnzKmWaY1s7fec3i5kGpB6dLDvXtbqZEbQ3KlRIm4Xka4Z8BAcCFBCr6uJ/KeNP6ZIlmz/DtNoz0BTPrutePe7dz6jTUCzB9UfuRN6xwW0cvABQUp+LoebnFJFlhRjqF5cfG28vJCxKIWd9Ri5WWg++g/6V1mXp78s+We4cwovjDYAGinOXqBw1/jc+UxixHL2KJWpGpiernYcleHtOjC8nVbv7XVdXE3s3UCAKxE9SvY+WcqKpJVSNGsOstlYZWr52SaFjnAz0nPzY+VxJVbyJI7IadznjNTyP8BAHWEfbqa0HjDbG7JcatQc0dok2jHvt7RC56VzkXT8jdb7heJGa1Zie8y7mvv1lyrFa4rGItr6UYbDbXYpMUS2nCcEr2NT5ZxoJBXhS3qg6XbIzHN2khC2v8khVTyLGW6k4j7K0JOsfYJ9uHyiykly09zLWgxudwf7JXjlK9xkBUN4L0/mJdNw/PpWvlHoiFmIOcs/JZZStefrqOo/OaaArWEJ9K4AEiN0otakNwKmo9TOwcnIoj6PnkRsWSd87w01wGvA6/epDxntyQ/lltvB897uDKKA++naRmw5b/UxJjvs3y43kSWdpx1rIYVpuWJr3ZuLc+aY2o7n5F8RhazlWoeUDPAwo3T5T5HKYwpEnlghVJp5/UEVZzQYg60Vn+a5g6RzsvF0etksvBPytPyf4+liB8BAPW40QtaONSxjW6XXAlR4eACFRm8ipYvc/C2PjlKimbQstIm2bxhJu/AeF1pkRLj+StB+flYvnozKDEAUap8uhQ+0SNZwFoelhXrhV55VAtUMb+eyqFZv5o4S24Dmpdl8c4huN/8/sZM/eGzlRKAeQkvjuBCIImOdRzFEhNrKF8b2RAim1/NbbRcXiRCYmmTcozmznmaw7Itpfy6bLQOCACg4/p06d/js2YNShZbEfalpIsJP6clzmWwrRUVZCl6QXLLtJ7zEfLfB+qyKaXMU8YHQcfTn7e2q6roBcma5dYeT8sjAYpwLCXqC/4cX39ZvNhcXhbNJVKS/Jt4Gfmx3ghBKsPTje9Thk8pPoJLOrunCzcJqIb+vLUju/SONO7TtdB8m9L+Fp+mdv6oyJWf/6y0WscinYtauPx31LgMZriZc+ITZ5bXGgBgUSW6luBEhsyeSPHPEZ+hNTlnpT+XLYuWszSxZ6XxzivVgSTCs/Xy3MLVRgw1I4ltwDpgQKiKXtCEtkXwJJHW7CfZDVFC6c7nOKeqEQrul/Z+O/Vn8zJZHZXn+x5PTqXI9f7h2A7L9wR8MIAQfvaCJjRRQeCCq4WgRaE3d8QS9nKLwifGtM5IKoNUh5Iot5XsXkopP89fqI1HAQBQQqIrRS9Ivsootf5fDyv9WQDrJtLoZ81ijVr5cf/y+bzPEQyEm8McB2B6mibStFCyyDEUKmSaz1OiTjj55Jjtt41s/8vbDxnTXAnRuF6e32h+Y3J/C6WUBobuK1nhsq5QRkrXV7BHoH5gnle08mr8htG0npBrHYz3+7kroqW+nqP8rDyjv+Lzt5Qj2qN8RTYUTBoprFkvK5R6hTJSmi3dFveC5Au+W1S4mKrWZfHT8HwiE3dH2og4X4mO6MZXIX5ic6n/vBTm380fYS6fX5AxaZTkK/mOennHr3iWf68c3Cq8Ndsj1JXhnLqk9CUWlq/VEtqW+Fuv8xneizsVqYkqxJby3rpYzaqckUuLI1Jqb141/mCLv0k9uTlw90KNT9dKF3UxSD7dyAQcGjcA7yTDBwcAAOO4bOkCAACIA9EFAICBQHQBAGAgEF0AABgIRBcAAAYC0QUAgIH8D82Fvuj73nZLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    pyplot.subplot(2, 3, 1 + i)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(photo_imgs[i].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "    # select a sample of input images\n",
    "    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "    # generate translated images\n",
    "    X_out, _ = g_model.generate_fake_samples(X_in, 0)\n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    X_in = (X_in + 1) / 2.0\n",
    "    X_out = (X_out + 1) / 2.0\n",
    "    # plot real images\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_in[i])\n",
    "    # plot translated image\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_out[i])\n",
    "    # save plot to file\n",
    "    filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_pool(pool, images, max_size=50):\n",
    "    selected = list()\n",
    "    for image in images:\n",
    "        if len(pool) < max_size:\n",
    "            # stock the pool\n",
    "            pool.append(image)\n",
    "            selected.append(image)\n",
    "        elif random() < 0.5:\n",
    "            # use image, but don't add it to the pool\n",
    "            selected.append(image)\n",
    "        else:\n",
    "            # replace an existing image and use replaced image\n",
    "            ix = randint(0, len(pool))\n",
    "            selected.append(pool[ix])\n",
    "            pool[ix] = image\n",
    "    return np.asarray(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "    # save the first generator model\n",
    "    filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
    "    g_model_AtoB.model.save(filename1)\n",
    "    # save the second generator model\n",
    "    filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
    "    g_model_BtoA.model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cyclegan models\n",
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n",
    "    # define properties of the training run\n",
    "    n_epochs, n_batch, = 10, 10\n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch = d_model_A.model.output_shape[1]\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    # prepare image pool for fakes\n",
    "    poolA, poolB = list(), list()\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # select a batch of real samples\n",
    "        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
    "        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeA, y_fakeA = g_model_BtoA.generate_fake_samples(X_realB, n_patch)\n",
    "        X_fakeB, y_fakeB = g_model_AtoB.generate_fake_samples(X_realA, n_patch)\n",
    "        # update fakes from pool\n",
    "        X_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "        X_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "        # update generator B->A via adversarial and cycle loss\n",
    "        g_loss2, _, _, _, _  = c_model_BtoA.model.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "        # update discriminator for A -> [real/fake]\n",
    "        dA_loss1 = d_model_A.model.train_on_batch(X_realA, y_realA)\n",
    "        dA_loss2 = d_model_A.model.train_on_batch(X_fakeA, y_fakeA)\n",
    "        # update generator A->B via adversarial and cycle loss\n",
    "        g_loss1, _, _, _, _ = c_model_AtoB.model.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "        # update discriminator for B -> [real/fake]\n",
    "        dB_loss1 = d_model_B.model.train_on_batch(X_realB, y_realB)\n",
    "        dB_loss2 = d_model_B.model.train_on_batch(X_fakeB, y_fakeB)\n",
    "        # summarize performance\n",
    "        print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % (int(bat_per_epo / 5)) == 0:\n",
    "            # plot A->B translation\n",
    "            summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n",
    "            # plot B->A translation\n",
    "            summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n",
    "        if (i+1) % (bat_per_epo * 5) == 0:\n",
    "            # save the models\n",
    "            save_models(i, g_model_AtoB, g_model_BtoA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[10,64,64,1792] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Photo-Generator_8/concatenate_33/concat (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_58455]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-059681b75df6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimpr_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoto_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-4f1a4a634811>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mX_fakeB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_image_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoolB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_fakeB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# update generator B->A via adversarial and cycle loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mg_loss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mc_model_BtoA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_realB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_realA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m# update discriminator for A -> [real/fake]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdA_loss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_realA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[10,64,64,1792] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Photo-Generator_8/concatenate_33/concat (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_58455]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "train(d1, d2, g1, g2, i2p, p2i, (impr_imgs, photo_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "summarize_performance(0, g2, photo_imgs, 'BtoA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
